{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm \n",
    "import torch\n",
    "from lora_adapters import LoraConv2d, apply_adapter, mark_only_lora_as_trainable, lora_state_dict, undo_lora\n",
    "from torch.optim import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = timm.create_model('resnet50', pretrained=True).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW((param for param in model.parameters() if param.requires_grad), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.randn(1, 3, 224, 224).to('cuda')\n",
    "targets = torch.randint(0, 1000, (1,)).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.085061073303223\n",
      "6.210545063018799\n",
      "3.2762057781219482\n",
      "0.6974539756774902\n",
      "0.05648316815495491\n",
      "0.01910405606031418\n",
      "0.00746177276596427\n",
      "0.0039986190386116505\n",
      "0.002394310897216201\n",
      "0.0014627005439251661\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(inputs)\n",
    "    loss = torch.nn.functional.cross_entropy(outputs, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_parameters = sum(p.numel() for p in model.parameters())    \n",
    "model_grads = sum(p.grad.numel() for p in model.parameters() if p.requires_grad)    \n",
    "optimizer_states = sum([sum(elem.numel() for elem in  p.values()) for p in optimizer.state.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = apply_adapter(model, LoraConv2d, rank=16)\n",
    "# model = mark_only_lora_as_trainable(model, bias='lora_only')\n",
    "optimizer = AdamW((param for param in model.parameters() if param.requires_grad), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000910344475414604\n",
      "0.0002079985715681687\n",
      "7.962863310240209e-05\n",
      "3.8980677345534787e-05\n",
      "2.1934269170742482e-05\n",
      "1.3589766240329482e-05\n",
      "9.417489309271332e-06\n",
      "6.556489552167477e-06\n",
      "4.887569048150908e-06\n",
      "3.6954811548639555e-06\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(inputs)\n",
    "    loss = torch.nn.functional.cross_entropy(outputs, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model_parameters = sum(p.numel() for p in model.parameters())    \n",
    "lora_model_grads = sum(p.grad.numel() for p in model.parameters() if p.requires_grad)    \n",
    "lora_optimizer_states = sum([sum(elem.numel() for elem in  p.values()) for p in optimizer.state.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 25557032 -> 26828120 ratio: 1.05\n",
      "Model grads: 25557032 -> 3373208 ratio: 0.13\n",
      "Optimizer states: 51114225 -> 6746630 ratio: 0.13\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model parameters: {model_parameters} -> {lora_model_parameters} ratio: {lora_model_parameters/model_parameters:.2f}\")\n",
    "print(f\"Model grads: {model_grads} -> {lora_model_grads} ratio: {lora_model_grads/model_grads:.2f}\")\n",
    "print(f\"Optimizer states: {optimizer_states} -> {lora_optimizer_states} ratio: {lora_optimizer_states/optimizer_states:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = undo_lora(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_outputs = model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.equal(output, normal_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = apply_adapter(model, LoraConv2d, rank=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = {k: v for k, v in state_dict.items() if 'lora_' in k}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['conv1.lora_A', 'conv1.lora_B', 'layer1.0.conv1.lora_A', 'layer1.0.conv1.lora_B', 'layer1.0.conv2.lora_A', 'layer1.0.conv2.lora_B', 'layer1.0.conv3.lora_A', 'layer1.0.conv3.lora_B', 'layer1.0.downsample.0.lora_A', 'layer1.0.downsample.0.lora_B', 'layer1.1.conv1.lora_A', 'layer1.1.conv1.lora_B', 'layer1.1.conv2.lora_A', 'layer1.1.conv2.lora_B', 'layer1.1.conv3.lora_A', 'layer1.1.conv3.lora_B', 'layer1.2.conv1.lora_A', 'layer1.2.conv1.lora_B', 'layer1.2.conv2.lora_A', 'layer1.2.conv2.lora_B', 'layer1.2.conv3.lora_A', 'layer1.2.conv3.lora_B', 'layer2.0.conv1.lora_A', 'layer2.0.conv1.lora_B', 'layer2.0.conv2.lora_A', 'layer2.0.conv2.lora_B', 'layer2.0.conv3.lora_A', 'layer2.0.conv3.lora_B', 'layer2.0.downsample.0.lora_A', 'layer2.0.downsample.0.lora_B', 'layer2.1.conv1.lora_A', 'layer2.1.conv1.lora_B', 'layer2.1.conv2.lora_A', 'layer2.1.conv2.lora_B', 'layer2.1.conv3.lora_A', 'layer2.1.conv3.lora_B', 'layer2.2.conv1.lora_A', 'layer2.2.conv1.lora_B', 'layer2.2.conv2.lora_A', 'layer2.2.conv2.lora_B', 'layer2.2.conv3.lora_A', 'layer2.2.conv3.lora_B', 'layer2.3.conv1.lora_A', 'layer2.3.conv1.lora_B', 'layer2.3.conv2.lora_A', 'layer2.3.conv2.lora_B', 'layer2.3.conv3.lora_A', 'layer2.3.conv3.lora_B', 'layer3.0.conv1.lora_A', 'layer3.0.conv1.lora_B', 'layer3.0.conv2.lora_A', 'layer3.0.conv2.lora_B', 'layer3.0.conv3.lora_A', 'layer3.0.conv3.lora_B', 'layer3.0.downsample.0.lora_A', 'layer3.0.downsample.0.lora_B', 'layer3.1.conv1.lora_A', 'layer3.1.conv1.lora_B', 'layer3.1.conv2.lora_A', 'layer3.1.conv2.lora_B', 'layer3.1.conv3.lora_A', 'layer3.1.conv3.lora_B', 'layer3.2.conv1.lora_A', 'layer3.2.conv1.lora_B', 'layer3.2.conv2.lora_A', 'layer3.2.conv2.lora_B', 'layer3.2.conv3.lora_A', 'layer3.2.conv3.lora_B', 'layer3.3.conv1.lora_A', 'layer3.3.conv1.lora_B', 'layer3.3.conv2.lora_A', 'layer3.3.conv2.lora_B', 'layer3.3.conv3.lora_A', 'layer3.3.conv3.lora_B', 'layer3.4.conv1.lora_A', 'layer3.4.conv1.lora_B', 'layer3.4.conv2.lora_A', 'layer3.4.conv2.lora_B', 'layer3.4.conv3.lora_A', 'layer3.4.conv3.lora_B', 'layer3.5.conv1.lora_A', 'layer3.5.conv1.lora_B', 'layer3.5.conv2.lora_A', 'layer3.5.conv2.lora_B', 'layer3.5.conv3.lora_A', 'layer3.5.conv3.lora_B', 'layer4.0.conv1.lora_A', 'layer4.0.conv1.lora_B', 'layer4.0.conv2.lora_A', 'layer4.0.conv2.lora_B', 'layer4.0.conv3.lora_A', 'layer4.0.conv3.lora_B', 'layer4.0.downsample.0.lora_A', 'layer4.0.downsample.0.lora_B', 'layer4.1.conv1.lora_A', 'layer4.1.conv1.lora_B', 'layer4.1.conv2.lora_A', 'layer4.1.conv2.lora_B', 'layer4.1.conv3.lora_A', 'layer4.1.conv3.lora_B', 'layer4.2.conv1.lora_A', 'layer4.2.conv1.lora_B', 'layer4.2.conv2.lora_A', 'layer4.2.conv2.lora_B', 'layer4.2.conv3.lora_A', 'layer4.2.conv3.lora_B'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_dict = {name:module for name, module in model.named_modules() if getattr(module, 'is_lora', False)}\n",
    "lora_dict_weights = {name:module.weight for name, module in lora_dict.items()}\n",
    "lora_dict_bias = {name:module.bias for name, module in lora_dict.items() if hasattr(module, 'bias') and module.bias is not None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['conv1.weight', 'layer1.0.conv1.weight', 'layer1.0.conv2.weight', 'layer1.0.conv3.weight', 'layer1.0.downsample.0.weight', 'layer1.1.conv1.weight', 'layer1.1.conv2.weight', 'layer1.1.conv3.weight', 'layer1.2.conv1.weight', 'layer1.2.conv2.weight', 'layer1.2.conv3.weight', 'layer2.0.conv1.weight', 'layer2.0.conv2.weight', 'layer2.0.conv3.weight', 'layer2.0.downsample.0.weight', 'layer2.1.conv1.weight', 'layer2.1.conv2.weight', 'layer2.1.conv3.weight', 'layer2.2.conv1.weight', 'layer2.2.conv2.weight', 'layer2.2.conv3.weight', 'layer2.3.conv1.weight', 'layer2.3.conv2.weight', 'layer2.3.conv3.weight', 'layer3.0.conv1.weight', 'layer3.0.conv2.weight', 'layer3.0.conv3.weight', 'layer3.0.downsample.0.weight', 'layer3.1.conv1.weight', 'layer3.1.conv2.weight', 'layer3.1.conv3.weight', 'layer3.2.conv1.weight', 'layer3.2.conv2.weight', 'layer3.2.conv3.weight', 'layer3.3.conv1.weight', 'layer3.3.conv2.weight', 'layer3.3.conv3.weight', 'layer3.4.conv1.weight', 'layer3.4.conv2.weight', 'layer3.4.conv3.weight', 'layer3.5.conv1.weight', 'layer3.5.conv2.weight', 'layer3.5.conv3.weight', 'layer4.0.conv1.weight', 'layer4.0.conv2.weight', 'layer4.0.conv3.weight', 'layer4.0.downsample.0.weight', 'layer4.1.conv1.weight', 'layer4.1.conv2.weight', 'layer4.1.conv3.weight', 'layer4.2.conv1.weight', 'layer4.2.conv2.weight', 'layer4.2.conv3.weight'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_state_dict(model, bias='lora_only').keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
