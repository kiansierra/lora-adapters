{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: timm in /home/kian/anaconda3/envs/torch-2/lib/python3.10/site-packages (0.6.13)\n",
      "Requirement already satisfied: huggingface-hub in /home/kian/anaconda3/envs/torch-2/lib/python3.10/site-packages (from timm) (0.13.3)\n",
      "Requirement already satisfied: pyyaml in /home/kian/anaconda3/envs/torch-2/lib/python3.10/site-packages (from timm) (6.0)\n",
      "Requirement already satisfied: torch>=1.7 in /home/kian/anaconda3/envs/torch-2/lib/python3.10/site-packages (from timm) (2.0.0)\n",
      "Requirement already satisfied: torchvision in /home/kian/anaconda3/envs/torch-2/lib/python3.10/site-packages (from timm) (0.15.0)\n",
      "Requirement already satisfied: filelock in /home/kian/anaconda3/envs/torch-2/lib/python3.10/site-packages (from torch>=1.7->timm) (3.10.7)\n",
      "Requirement already satisfied: typing-extensions in /home/kian/anaconda3/envs/torch-2/lib/python3.10/site-packages (from torch>=1.7->timm) (4.5.0)\n",
      "Requirement already satisfied: sympy in /home/kian/anaconda3/envs/torch-2/lib/python3.10/site-packages (from torch>=1.7->timm) (1.11.1)\n",
      "Requirement already satisfied: networkx in /home/kian/anaconda3/envs/torch-2/lib/python3.10/site-packages (from torch>=1.7->timm) (3.0)\n",
      "Requirement already satisfied: jinja2 in /home/kian/anaconda3/envs/torch-2/lib/python3.10/site-packages (from torch>=1.7->timm) (3.1.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/kian/anaconda3/envs/torch-2/lib/python3.10/site-packages (from huggingface-hub->timm) (23.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/kian/anaconda3/envs/torch-2/lib/python3.10/site-packages (from huggingface-hub->timm) (4.65.0)\n",
      "Requirement already satisfied: requests in /home/kian/anaconda3/envs/torch-2/lib/python3.10/site-packages (from huggingface-hub->timm) (2.28.2)\n",
      "Requirement already satisfied: numpy in /home/kian/anaconda3/envs/torch-2/lib/python3.10/site-packages (from torchvision->timm) (1.24.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/kian/anaconda3/envs/torch-2/lib/python3.10/site-packages (from torchvision->timm) (9.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/kian/anaconda3/envs/torch-2/lib/python3.10/site-packages (from jinja2->torch>=1.7->timm) (2.1.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/kian/anaconda3/envs/torch-2/lib/python3.10/site-packages (from requests->huggingface-hub->timm) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/kian/anaconda3/envs/torch-2/lib/python3.10/site-packages (from requests->huggingface-hub->timm) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/kian/anaconda3/envs/torch-2/lib/python3.10/site-packages (from requests->huggingface-hub->timm) (3.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/kian/anaconda3/envs/torch-2/lib/python3.10/site-packages (from requests->huggingface-hub->timm) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/kian/anaconda3/envs/torch-2/lib/python3.10/site-packages (from sympy->torch>=1.7->timm) (1.3.0)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: lora_adapters in /home/kian/anaconda3/envs/torch-2/lib/python3.10/site-packages (0.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install timm\n",
    "!pip install lora_adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm \n",
    "import torch\n",
    "from lora_adapters import LoraConv2d, apply_adapter, mark_only_lora_as_trainable, lora_state_dict, undo_lora\n",
    "from torch.optim import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = timm.create_model('resnet50', pretrained=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW((param for param in model.parameters() if param.requires_grad), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.randn(1, 3, 224, 224).to(device)\n",
    "targets = torch.randint(0, 1000, (1,)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 7.22588\n",
      "loss: 6.38845\n",
      "loss: 3.75152\n",
      "loss: 0.89187\n",
      "loss: 0.17836\n",
      "loss: 0.02842\n",
      "loss: 0.01040\n",
      "loss: 0.00516\n",
      "loss: 0.00258\n",
      "loss: 0.00150\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(inputs)\n",
    "    loss = torch.nn.functional.cross_entropy(outputs, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"loss: {loss.item():.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_parameters = sum(p.numel() for p in model.parameters())    \n",
    "model_grads = sum(p.grad.numel() for p in model.parameters() if p.requires_grad)    \n",
    "optimizer_states = sum([sum(elem.numel() for elem in  p.values()) for p in optimizer.state.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = apply_adapter(model, LoraConv2d, rank=16)\n",
    "# We Train all the parameters to compare with the original model\n",
    "# model = mark_only_lora_as_trainable(model, bias='lora_only') \n",
    "optimizer = AdamW((param for param in model.parameters() if param.requires_grad), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.00096\n",
      "loss: 0.00029\n",
      "loss: 0.00013\n",
      "loss: 0.00007\n",
      "loss: 0.00004\n",
      "loss: 0.00003\n",
      "loss: 0.00002\n",
      "loss: 0.00001\n",
      "loss: 0.00001\n",
      "loss: 0.00001\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(inputs)\n",
    "    loss = torch.nn.functional.cross_entropy(outputs, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"loss: {loss.item():.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model_parameters = sum(p.numel() for p in model.parameters())    \n",
    "lora_model_grads = sum(p.grad.numel() for p in model.parameters() if p.requires_grad)    \n",
    "lora_optimizer_states = sum([sum(elem.numel() for elem in  p.values()) for p in optimizer.state.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 25557032 -> 27360600 ratio: 1.07\n",
      "Model grads: 25557032 -> 3905688 ratio: 0.15\n",
      "Optimizer states: 51114225 -> 7811590 ratio: 0.15\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model parameters: {model_parameters} -> {lora_model_parameters} ratio: {lora_model_parameters/model_parameters:.2f}\")\n",
    "print(f\"Model grads: {model_grads} -> {lora_model_grads} ratio: {lora_model_grads/model_grads:.2f}\")\n",
    "print(f\"Optimizer states: {optimizer_states} -> {lora_optimizer_states} ratio: {lora_optimizer_states/optimizer_states:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
